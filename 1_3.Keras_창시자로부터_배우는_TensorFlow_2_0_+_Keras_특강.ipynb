{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Keras 창시자로부터 배우는 TensorFlow 2.0 + Keras 특강.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTuxi-1k1F2C",
        "colab_type": "code",
        "outputId": "18e8f48d-585c-4648-d274-9f0cc2f17252",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 895
        }
      },
      "source": [
        "!pip install tensorflow==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n",
            "\u001b[K     |████████████████████████████████| 86.3MB 1.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.0.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 32.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 25.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\n",
            "Collecting gast==0.2.2 (from tensorflow==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=a25e65ef6c475e1faaf923ba1f0657029e5add8c21b7e87d17e138427c732efa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "Installing collected packages: tensorflow-estimator, tensorboard, gast, tensorflow\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W15eefa2Q5hA",
        "colab_type": "code",
        "outputId": "b26ae8e8-17ca-487e-fe98-96eff892149f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoDjozMFREDU",
        "colab_type": "text"
      },
      "source": [
        "# TensorFlow 2.0 + Keras, 딥러닝 연구자들을 위한 오버뷰\n",
        "\n",
        "*@fchollet, October 2019 (번역 @chansung)*\n",
        "- 원본은 [TensorFlow 2.0 + Keras Overview for Deep Learning Researchers](https://colab.research.google.com/drive/1UCJt8EYjlzCs1H1d1X0iDGYJsHKwu-NO?fbclid=IwAR269Y-3J1DuZL01L6GBCC4dg6RSAmJXHnRfztL454dZ5SqKLRxCAZcxzgY)입니다.\n",
        "---\n",
        "\n",
        "**이 문서는 입문, 특강, 그리고 TensorFlow 2.0의 API를 빠르게 참조하는 목적을 위해 제공됩니다.**\n",
        "\n",
        "---\n",
        "\n",
        "TensorFlow와 Keras는 모두 약 4년전쯤 릴리즈 되었습니다 (Keras는 2015년 3월, TensorFlow는 2015년 11월). 이는 딥러닝 세계의 관점에서 볼 때, 꽤 오랜시간이라고 볼 수 있습니다!\n",
        "\n",
        "과거에 TensorFlow 1.x + Keras는 여러가지 알려진 문제점을 가지고 있었습니다:\n",
        "- TensorFlow를 사용한다는것은 정적인 계산 그래프를 조작함을 의미하는것으로, Imperative 코딩 스타일을 사용하는 프로그래머로 하여금 어렵고, 불편한 느낌을 받게 했었습니다.\n",
        "- TensorFlow API가 매우 강력하면서도 유연하지만, 빠른 코드의 작성의 가능성이 결여되어 있었으며 종종 사용법은 어렵고 혼란스러웠습니다.\n",
        "- Keras는 매우 생산적이고 사용이 쉽지만, 연구에 사용된 사례에서 종종 유연성이 결여되었었습니다.\n",
        "\n",
        "---\n",
        "### TensorFlow 2.0은 TensorFlow와 Keras를 대대적으로 새로이 디자인한 것으로, 지난 4년간의 사용자 피드백과 기술의 진보가 모두 고려되었습니다. 위에서 언급된 문제점들을 대규모로 수정합니다.\n",
        "\n",
        "### 미래에서온 차세대 머신러닝 플랫폼입니다\n",
        "\n",
        "---\n",
        "\n",
        "TensorFlow 2.0은 아래와 같은 주요 아이디어에 기반하고 있습니다:\n",
        "\n",
        "- 사용자들이 계산을 eagerly하게 수행할 수 있게 해줍니다. 이는 Numpy를 사용하는법과 유사합니다. 이는 TensorFlow 2.0을 이용한 프로그래밍이 직관적이며 동시에 파이토닉할 수 있게끔 해 줍니다.\n",
        "- 컴파일된 그래프의 엄청난 이점을 그대로 보존하는데, 이는 성능, 분산, 그리고 배포를 위함입니다. 이 내용은 TensorFlow를 빠르고, 분산 구조에서의 확장 가능하며, 상용화에 준비될 수 있도록 해 줍니다.\n",
        "- Keras를 딥러닝의 고수준 API로 채택하여, TensorFlow를 이해하기 쉬우면서도 높은 생산성을 가질 수 있게 만들어 줍니다.\n",
        "- 매우 고수준(더 쉬운 사용성, 약간 부족한 유연성) 에서부터 매우 저수준(더 깊은 전문성, 매우 뛰어난 유연성)의 다양한 범위의 작업으로까지 Keras를 확장합니다.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U71NYDeFkUhq",
        "colab_type": "text"
      },
      "source": [
        "# 파트 1: TensorFlow의 기본"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l2e8-qrcl2kH",
        "colab_type": "text"
      },
      "source": [
        "## Tensors (텐서)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PX6JvH4h0zyY",
        "colab_type": "text"
      },
      "source": [
        "다음은 [상수형](https://www.tensorflow.org/api_docs/python/tf/constant) 텐서 입니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGB6GDsfRFJs",
        "colab_type": "code",
        "outputId": "12be0b11-2979-467c-e59d-b94b2c32bb1c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "x = tf.constant([[5, 2], [1, 3]])\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[5 2]\n",
            " [1 3]], shape=(2, 2), dtype=int32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX2SB_2O1jx7",
        "colab_type": "text"
      },
      "source": [
        "해당 텐서의 값을 Numpy 배열형태로 가져오고 싶다면 `.numpy()`를 호출하면 됩니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwGyHOoq1oWn",
        "colab_type": "code",
        "outputId": "42132234-b492-424c-c6f5-913eb8510005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "x.numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[5, 2],\n",
              "       [1, 3]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNkno66r1xvg",
        "colab_type": "text"
      },
      "source": [
        "Numpy 배열과 *꽤나* 유사한 점으로 `dtype`과 `shape`이라는 속성을 가집니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSxtblSP13v2",
        "colab_type": "code",
        "outputId": "43a0f40f-7b91-45ce-e410-7aab308fca6e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "print('dtype:', x.dtype)\n",
        "print('shape:', x.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dtype: <dtype: 'int32'>\n",
            "shape: (2, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oogzv3--2EF2",
        "colab_type": "text"
      },
      "source": [
        "상수형 텐서를 생성하는 보편적인 방법은 `tf.ones`과 `tf.zeros`를 사용하는 것입니다(이는 Numpy의 `np.ones` 및 `np.zeros`와 유사합니다):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qDlfa8r2Lia",
        "colab_type": "code",
        "outputId": "5392afec-2628-4a3c-bf88-dce630c91e4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "print(tf.ones(shape=(2, 1)))\n",
        "print(tf.zeros(shape=(2, 1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.]\n",
            " [1.]], shape=(2, 1), dtype=float32)\n",
            "tf.Tensor(\n",
            "[[0.]\n",
            " [0.]], shape=(2, 1), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzGYEkdcmYbe",
        "colab_type": "text"
      },
      "source": [
        "## 랜덤한 상수형 텐서"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fk94gREJ2r-e",
        "colab_type": "text"
      },
      "source": [
        "다음은 랜덤한 [정규분포](https://www.tensorflow.org/api_docs/python/tf/random/normal)로부터 상수를 생성합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqRrO-Puma7-",
        "colab_type": "code",
        "outputId": "9cb745dd-9437-4041-b1bf-0abd209b0a0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "tf.random.normal(shape=(2, 2), mean=0., stddev=1.)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=12, shape=(2, 2), dtype=float32, numpy=\n",
              "array([[ 0.5702444 , -0.9350667 ],\n",
              "       [-0.25505924, -0.27178827]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wL0EMPT93SEU",
        "colab_type": "text"
      },
      "source": [
        "*그리고* 다음은 랜덤한 [균등분포](https://www.tensorflow.org/api_docs/python/tf/random/uniform)로부터 값이 채워지는 정수형 텐서를 보여줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9syARhtj2wbx",
        "colab_type": "code",
        "outputId": "734440ae-2607-447f-f9d6-46372aedcd5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "tf.random.uniform(shape=(2, 2), minval=0, maxval=10, dtype='int32')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=16, shape=(2, 2), dtype=int32, numpy=\n",
              "array([[2, 2],\n",
              "       [5, 1]], dtype=int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I95066exmbDU",
        "colab_type": "text"
      },
      "source": [
        "## Variables (변수)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMflzgPM3Mim",
        "colab_type": "text"
      },
      "source": [
        "[Variables](https://www.tensorflow.org/guide/variable)는 변할 수 있는 상태(뉴럴넷의 가중치와 같은)를 저장하는데 사용되는 특별한 텐서 입니다. 초기값을 사용해서 Variable을 생성할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FvENXmBmcyT",
        "colab_type": "code",
        "outputId": "b06fd6d6-5d85-458b-b4b1-74c89e9e3679",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "initial_value = tf.random.normal(shape=(2, 2))\n",
        "a = tf.Variable(initial_value)\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=\n",
            "array([[ 0.4541328 , -0.49918348],\n",
            "       [-1.0928912 ,  0.8149931 ]], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRFwVySi3biu",
        "colab_type": "text"
      },
      "source": [
        "`.assign(value)`, `.assign_add(increment)`, 또는 `.assign_sub(decrement)`와 같은 메소드를 사용해서 Variable의 값을 갱신합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOCsCNvc3mNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_value = tf.random.normal(shape=(2, 2))\n",
        "a.assign(new_value)\n",
        "for i in range(2):\n",
        "  for j in range(2):\n",
        "    assert a[i, j] == new_value[i, j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrSjwl_056j8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "added_value = tf.random.normal(shape=(2, 2))\n",
        "a.assign_add(added_value)\n",
        "for i in range(2):\n",
        "  for j in range(2):\n",
        "    assert a[i, j] == new_value[i, j] + added_value[i, j]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rAIqYQmOl_wR",
        "colab_type": "text"
      },
      "source": [
        "## TensorFlow에서 수학을 하는것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bmtTepn6SvG",
        "colab_type": "text"
      },
      "source": [
        "TensorFlow는 Numpy를 사용하는것과 정확히 똑같은 방법으로 사용할 수 있습니다. 이 둘의 주요 다른점은 작성한 TensorFlow의 코드는 GPU와 TPU 상에서 실행될 수 있다는 점입니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pCZGHQ_XmHuZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = tf.random.normal(shape=(2, 2))\n",
        "b = tf.random.normal(shape=(2, 2))\n",
        "\n",
        "c = a + b\n",
        "d = tf.square(c)\n",
        "e = tf.exp(d)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Feq3qWoBVQW",
        "colab_type": "text"
      },
      "source": [
        "## `GradientTape`을 사용해서 경사도를 계산하는것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JdsmOcrJBWXe",
        "colab_type": "text"
      },
      "source": [
        "한 가지 더 Numpy와의 큰 차이점이 있습니다: 모든 미분가능한 표현에 대해서, 자동으로 경사도를 구하는 것이 가능합니다.\n",
        "\n",
        "단순히 [`GradientTape`](https://www.tensorflow.org/api_docs/python/tf/GradientTape)를 열게되면, 그때부턴 `tape.watch()`를 통해 텐서를 확인하고, 이 텐서를 입력으로써 사용하는 미분가능한 표현을 구성하는것이 가능합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkEAY45IBjPv",
        "colab_type": "code",
        "outputId": "a012f34e-a876-42b7-9f5e-0a81f6223f91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "a = tf.random.normal(shape=(2, 2))\n",
        "b = tf.random.normal(shape=(2, 2))\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(a)  # `a`에 적용되는 연산의 히스토리에 대한 기록을 시작\n",
        "  c = tf.sqrt(tf.square(a) + tf.square(b))  # `a`를 사용하여 몇 가지 수학을 수행\n",
        "  # `a`에 대한 `c`의 경사도는 무엇인가?\n",
        "  dc_da = tape.gradient(c, a)\n",
        "  print(dc_da)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.9329647  0.9084445 ]\n",
            " [0.8140862  0.38161793]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8UpqFx_DDbV",
        "colab_type": "text"
      },
      "source": [
        "디폴트로는 Variable들은 자동으로 watch가 적용되어 있기 때문에, 수동으로 `watch`를 해 줄 필요는 없습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OtH3FuvDDOAY",
        "colab_type": "code",
        "outputId": "1d60932a-cedb-4ea9-bec6-5560213165b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "a = tf.Variable(a)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "  dc_da = tape.gradient(c, a)\n",
        "  print(dc_da)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.9329647  0.9084445 ]\n",
            " [0.8140862  0.38161793]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFlBGjuEDbt-",
        "colab_type": "text"
      },
      "source": [
        "GradientTape을 중첩시켜서 고차원의 미분을 계산할 수도 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjPcY0OIDhEA",
        "colab_type": "code",
        "outputId": "fa3c9e2a-6db4-4aa0-88bd-d186feec605a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "with tf.GradientTape() as outer_tape:\n",
        "  with tf.GradientTape() as tape:\n",
        "    c = tf.sqrt(tf.square(a) + tf.square(b))\n",
        "    dc_da = tape.gradient(c, a)\n",
        "  d2c_da2 = outer_tape.gradient(dc_da, a)\n",
        "  print(d2c_da2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[0.10599959 0.05318242]\n",
            " [0.28099984 0.6791785 ]], shape=(2, 2), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KC5RgwGeBP-9",
        "colab_type": "text"
      },
      "source": [
        "## end-to-end 예제: 선형 회귀"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Owbx4mlEErNN",
        "colab_type": "text"
      },
      "source": [
        "지금까지 TensorFlow는 Numpy와 비슷한 라이브러리인데, 추가적으로 GPU 또는 TPU를 통해 가속될 수 있고, 자동으로 미분이 계산된다는 내용을 배웠습니다. 그러면 이제는 end-to-end 예제를 알아볼 시간입니다: 머신러닝의 피즈버즈인, 선형 회귀를 구현해 봅시다. \n",
        "\n",
        "이를 보여주기 위해서, `Layer` 또는 `MeanSquaredError`와 같은 Keras의 고수준 컴포넌트를 사용하지 않을 것입니다. 단지 기본적인 연산자만을 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhitqoj2FH8U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 2\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "\n",
        "# 가중치 행렬입니다\n",
        "w = tf.Variable(tf.random.uniform(shape=(input_dim, output_dim)))\n",
        "# 편향 벡터입니다\n",
        "b = tf.Variable(tf.zeros(shape=(output_dim,)))\n",
        "\n",
        "def compute_predictions(features):\n",
        "  return tf.matmul(features, w) + b\n",
        "\n",
        "def compute_loss(labels, predictions):\n",
        "  return tf.reduce_mean(tf.square(labels - predictions))\n",
        "\n",
        "def train_on_batch(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = compute_predictions(x)\n",
        "    loss = compute_loss(y, predictions)\n",
        "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
        "  w.assign_sub(learning_rate * dloss_dw)\n",
        "  b.assign_sub(learning_rate * dloss_db)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qC1fp3BYJeXo",
        "colab_type": "text"
      },
      "source": [
        "작성한 모델을 검증하기 위한, 인공적인 데이터를 생성해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ocAkrliMVAQ",
        "colab_type": "code",
        "outputId": "60541e74-a30e-4e98-c713-95aa2c011cd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# 데이터셋을 준비합니다\n",
        "num_samples = 10000\n",
        "negative_samples = np.random.multivariate_normal(\n",
        "    mean=[0, 3], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
        "positive_samples = np.random.multivariate_normal(\n",
        "    mean=[3, 0], cov=[[1, 0.5],[0.5, 1]], size=num_samples)\n",
        "features = np.vstack((negative_samples, positive_samples)).astype(np.float32)\n",
        "labels = np.vstack((np.zeros((num_samples, 1), dtype='float32'),\n",
        "                    np.ones((num_samples, 1), dtype='float32')))\n",
        "\n",
        "plt.scatter(features[:, 0], features[:, 1], c=labels[:, 0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f0de1a92f98>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCdZTpjlJlGo",
        "colab_type": "text"
      },
      "source": [
        "그러면, 데이터의 배치크기 단위로 돌면서, `train_on_batch` 함수를 반복적으로 호출하여 선형 회귀 모델을 학습시켜 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YsHszjjaJDQZ",
        "colab_type": "code",
        "outputId": "7d8b1e4d-72b7-4c79-b461-92c5d2952929",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# 데이터를 무작위로 섞습니다\n",
        "random.Random(1337).shuffle(features)\n",
        "random.Random(1337).shuffle(labels)\n",
        "\n",
        "# 손쉽게 배치화된 반복을 위해, tf.data.Dataset 객체를 생성합니다\n",
        "dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(256)\n",
        "\n",
        "for epoch in range(10):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = train_on_batch(x, y)\n",
        "  print('Epoch %d: 마지막 배치의 손실값 = %.4f' % (epoch, float(loss)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0: 마지막 배치의 손실값 = 0.0861\n",
            "Epoch 1: 마지막 배치의 손실값 = 0.0771\n",
            "Epoch 2: 마지막 배치의 손실값 = 0.0546\n",
            "Epoch 3: 마지막 배치의 손실값 = 0.0217\n",
            "Epoch 4: 마지막 배치의 손실값 = 0.0327\n",
            "Epoch 5: 마지막 배치의 손실값 = 0.0262\n",
            "Epoch 6: 마지막 배치의 손실값 = 0.0282\n",
            "Epoch 7: 마지막 배치의 손실값 = 0.0407\n",
            "Epoch 8: 마지막 배치의 손실값 = 0.0260\n",
            "Epoch 9: 마지막 배치의 손실값 = 0.0260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIDDhTcyJwSM",
        "colab_type": "text"
      },
      "source": [
        "아래는 우리가 만든 모델이 얼마나 잘 동작하는지를 보여줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBPYQpskJxxT",
        "colab_type": "code",
        "outputId": "40e15832-4069-46d0-a8a9-2d92bf94d5f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 288
        }
      },
      "source": [
        "predictions = compute_predictions(features)\n",
        "plt.scatter(features[:, 0], features[:, 1], c=predictions[:, 0] > 0.5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f0dde1c6c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBcqiop7mH7x",
        "colab_type": "text"
      },
      "source": [
        "## `tf.function`를 이용해서 속도를 빠르게 하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kjZ8kuruNdj6",
        "colab_type": "text"
      },
      "source": [
        "현재의 코드는 얼마나 빨리 수행될까요?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXoe7S5RmStB",
        "colab_type": "code",
        "outputId": "0df98598-fc95-4b47-abb8-0eb70d06fe4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "import time\n",
        "\n",
        "t0 = time.time()\n",
        "for epoch in range(20):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = train_on_batch(x, y)\n",
        "t_end = time.time() - t0\n",
        "print('epoch당 걸린 시간: %.3f 초' % (t_end / 20,))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch당 걸린 시간: 0.186 초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHsvPqyRN_E3",
        "colab_type": "text"
      },
      "source": [
        "학습 함수를 정적 그래프로 컴파일 해 봅시다. 이를 위해서 해야할 것은 문자 그대로, `tf.function`이라는 데코레이터를 위에 넣어주는것 뿐입니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oEYFkThcOGcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_on_batch(x, y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    predictions = compute_predictions(x)\n",
        "    loss = compute_loss(y, predictions)\n",
        "    dloss_dw, dloss_db = tape.gradient(loss, [w, b])\n",
        "  w.assign_sub(learning_rate * dloss_dw)\n",
        "  b.assign_sub(learning_rate * dloss_db)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ocOuskvoOKsx",
        "colab_type": "text"
      },
      "source": [
        "다시한번 시간을 측정해 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KT2w6DVmONB5",
        "colab_type": "code",
        "outputId": "23b2b44b-624f-4484-bf12-dc82940c49b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "t0 = time.time()\n",
        "for epoch in range(20):\n",
        "  for step, (x, y) in enumerate(dataset):\n",
        "    loss = train_on_batch(x, y)\n",
        "t_end = time.time() - t0\n",
        "print('epoch당 걸린 시간: %.3f 초' % (t_end / 20,))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch당 걸린 시간: 0.092 초\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YYPWZaSqOfEL",
        "colab_type": "text"
      },
      "source": [
        "걸린 시간이 약 40% 감소했습니다. 이 경우, 매우 간단한 모델을 사용했습니다; 일반적으로 모델이 크면 클 수록, 정적 그래프를 활용한 속도 개선은 더 많이 이뤄집니다.\n",
        "\n",
        "기억해야할 것이 있습니다: eager 실행모드는 디버깅과 코드 라인별 결과를 출력하는데 매우 유용하지만, 크기를 키워야할 시기가 오면, 정적 그래프가 연구자들에게 최고의 친구가 될 것입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3I3v_FqjFty",
        "colab_type": "text"
      },
      "source": [
        "# 파트 2: Keras API"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjLI719fPfJi",
        "colab_type": "text"
      },
      "source": [
        "Keras는 딥러닝을 위한 파이썬 API 입니다. 모두가 사용할만한 내용을 가지고 있습니다:\n",
        "\n",
        "- 엔지니어의 경우, Keras는 계층, 평가지표(metrics), 학습 반복문과 같은 재사용 가능한 블록을 제공하여 일반적은 사용 사례를 지원합니다. 고수준의 사용자 경험을 제공하여 접근이 용이하고, 생산성이 좋습니다.\n",
        "\n",
        "- 연구자의 경우, 계층이나 학습 반목문과 같은 이미 제공되는 블록의 사용을 선호하지 않고, 스스로 만든 것을 대신 사용할 지도 모릅니다. 물론, Keras는 이를 가능하게 해 줍니다. 이 경우, Keras는 여러분이 작성하게될 블록에 대한 템플릿을 Layers 및 Metrics와 같은 표준적인 API와 함께 제공합니다. 이러한 구조는 다른 사람과 코드를 쉽게 공유하고, 상용의 작업 흐름에도 통합될 수 있게끔 해 줍니다.\n",
        "\n",
        "- 이 같은 내용은 라이브러리를 개발하는 분들에게도 적용되는 사실입니다. TensorFlow는 거대한 생태계죠. 수 많은 라이브러리가 존재합니다. 서로다른 라이브러리가 상호작용하고, 이들의 컴포넌트를 공유할 수 있게하기 위해선 API 표준을 따라야만 합니다. API 표준이 곧 Keras가 제공하는 핵심입니다.\n",
        "\n",
        "\n",
        "Keras는 결정적으로 고수준의 UX와 저수준의 유연성을 모두 함께 완만히 도입합니다. 이는 더이상 한편으론 사용성이 뛰어나지만 유연치는 못한 고수준 API를, 다른 한편으론 매우 유연하지만 전문가만이 사용가능한 저수준 API를 가져야만 하는 상황에서 벗어나게 해 줍니다. 그 대신, 매우 고수준에서부터 매우 저수준 까지의 다양한 작업 흐름의 범위를 가질 수 있게 됩니다. 이 작업흐름이란, 동일한 컨셉과 객체에 기반해서 만들어졌기 때문에 모든것이 상호 호환 가능한 것을 의미합니다.\n",
        "\n",
        "![Keras 작업 흐름의 범위](https://drive.google.com/uc?export=view&id=1bE0FiQY2XF5QzBLRHfe7-SdxvwGIO0GK)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9DSVjdHPkOw",
        "colab_type": "text"
      },
      "source": [
        "## `Layer` 기본 클래스\n",
        "\n",
        "가장 첫 번째로 알아야할 클래스는 [`Layer`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Layer) 입니다. Keras의 거의 모든것은 이 클래스로부터 파생됩니다.\n",
        "\n",
        "Layer는 상태(가중치, weights)와 몇 (`call` 메소드에 정의된)계산을 캡슐화 합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Io3dUQzaPnPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer\n",
        "\n",
        "class Linear(Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32, input_dim=32):\n",
        "      super(Linear, self).__init__()\n",
        "      w_init = tf.random_normal_initializer()\n",
        "      self.w = tf.Variable(\n",
        "          initial_value=w_init(shape=(input_dim, units), dtype='float32'),\n",
        "          trainable=True)\n",
        "      b_init = tf.zeros_initializer()\n",
        "      self.b = tf.Variable(\n",
        "          initial_value=b_init(shape=(units,), dtype='float32'),\n",
        "          trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "# 우리가 만든 Layer객체를 인스턴스화 합니다\n",
        "linear_layer = Linear(4, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo3etyK8BO4a",
        "colab_type": "text"
      },
      "source": [
        "Layer 인스턴스는 마치 함수처럼 동작합니다. 몇 데이터에 대해서 이를 호출해 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBUCLfHVBQLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert y.shape == (2, 4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MXqpznsxBaCC",
        "colab_type": "text"
      },
      "source": [
        "`Layer` 클래스는 속성으로써 부여된 weights를 통해서, 가중치들을 추적합니다 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j_FaUtEYBeJw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 가중치는 자동으로 `weights`라는 속성으로써 추적됩니다.\n",
        "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6PZ6QXUHdxA",
        "colab_type": "text"
      },
      "source": [
        "`add_weight`를 이용하여 간단히 가중치를 생성하는 방법이 있는것도 알아두세요. 이렇게 코드를 작성하는것 대신:\n",
        "\n",
        "```python\n",
        "w_init = tf.random_normal_initializer()\n",
        "self.w = tf.Variable(initial_value=w_init(shape=shape, dtype='float32'))\n",
        "```\n",
        "\n",
        "일반적으로 아래와 같이 작성합니다:\n",
        "\n",
        "```python\n",
        "self.w = self.add_weight(shape=shape, initializer='random_normal')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lphpMGIiHRUP",
        "colab_type": "text"
      },
      "source": [
        "`build`라는 별도의 메소드에서 가중치를 생성하는것이 좋은 관례입니다. 이 `build`는 Layer에 의해 첫 번째 입력의 Shape이 확인되는 순간 호출되는 lazy한 메소드 입니다. 이러한 패턴은 입력 차원(input_dim)을 생성자에 명시하지 않아도 되게 해 줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpPjScZKHXhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Layer):\n",
        "  \"\"\"y = w.x + b\"\"\"\n",
        "\n",
        "  def __init__(self, units=32):\n",
        "      super(Linear, self).__init__()\n",
        "      self.units = units\n",
        "\n",
        "  def build(self, input_shape):\n",
        "      self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "      self.b = self.add_weight(shape=(self.units,),\n",
        "                               initializer='random_normal',\n",
        "                               trainable=True)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      return tf.matmul(inputs, self.w) + self.b\n",
        "\n",
        "\n",
        "# Lazy한 Layer의 인스턴스를 만듭니다.\n",
        "linear_layer = Linear(4)\n",
        "\n",
        "# 이렇게 하면, `build(input_shape)`이 호출되어 가중치를 생성하게 됩니다.\n",
        "y = linear_layer(tf.ones((2, 2)))\n",
        "assert len(linear_layer.weights) == 2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86khdsF3Pnr0",
        "colab_type": "text"
      },
      "source": [
        "## 학습 가능한, 그리고 학습 불가능한 가중치"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32zvCEKLICr5",
        "colab_type": "text"
      },
      "source": [
        "Layer에 의해 생성된 가중치는 학습이 가능할 수도, 학습이 불가능할 수도 있습니다. 이 두 경우는 각각 \n",
        "`trainable_weights` 및 `non_trainable_weights`로써 노출되어 외부에서 접근 가능합니다. 다음은 학습 불가능한 가중치를 가지는 Layer를 보여줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZ8s28NnX20u",
        "colab_type": "code",
        "outputId": "b8f608dc-e3ee-4655-8035-4b2d2ce3be2b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "class ComputeSum(Layer):\n",
        "  \"\"\"입력의 합산 결과를 반환하는 Layer\"\"\"\n",
        "\n",
        "  def __init__(self, input_dim):\n",
        "      super(ComputeSum, self).__init__()\n",
        "      # 학습 불가능한 가중치를 생성합니다.\n",
        "      self.total = tf.Variable(initial_value=tf.zeros((input_dim,)),\n",
        "                               trainable=False)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
        "      return self.total  \n",
        "\n",
        "my_sum = ComputeSum(2)\n",
        "x = tf.ones((2, 2))\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [2. 2.]\n",
        "\n",
        "y = my_sum(x)\n",
        "print(y.numpy())  # [4. 4.]\n",
        "\n",
        "assert my_sum.weights == [my_sum.total]\n",
        "assert my_sum.non_trainable_weights == [my_sum.total]\n",
        "assert my_sum.trainable_weights == []"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2. 2.]\n",
            "[4. 4.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oBkX6ZfYO8j",
        "colab_type": "text"
      },
      "source": [
        "## 재귀적으로 Layer를 조합하는것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YeLDL9MJI2dK",
        "colab_type": "text"
      },
      "source": [
        "Layer들은 더 큰 계산을 위한 블록을 생성하기 위해 재귀적으로 중첩될 수 있습니다. 각각의 Layer는 각각의 (학습 가능한것과 학습 불가능한)가중치를 추적할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5HBH-dtYQuk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `build` 메소드와 함께 앞서 정의된\n",
        "# Linear 클래스를 재사용 해봅시다\n",
        "\n",
        "class MLP(Layer):\n",
        "    \"\"\"Linear Layer의 간단한 층을 쌓는 Layer 입니다.\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.linear_1 = Linear(32)\n",
        "        self.linear_2 = Linear(32)\n",
        "        self.linear_3 = Linear(10)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.linear_1(inputs)\n",
        "        x = tf.nn.relu(x)\n",
        "        x = self.linear_2(x)\n",
        "        x = tf.nn.relu(x)\n",
        "        return self.linear_3(x)\n",
        "\n",
        "mlp = MLP()\n",
        "\n",
        "# `mlp` 객체에 대한 첫 번째 호출은 가중치를 생성하게 됩니다.\n",
        "y = mlp(tf.ones(shape=(3, 64)))\n",
        "\n",
        "# 가중치들은 재귀적으로 추적됩니다.\n",
        "assert len(mlp.weights) == 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WavMVtXGQk-z",
        "colab_type": "text"
      },
      "source": [
        "## 미리 정의된 Layer의 종류\n",
        "\n",
        "Keras는 [넓은 범위의 미리 정의된 Layer의 종류](https://www.tensorflow.org/api_docs/python/tf/keras/layers/)를 제공하여 항상 여러분 스스로가 모든것을 구현하지 않아도 되도록끔 해 줍니다.\n",
        "\n",
        "- Convolution layers\n",
        "- Transposed convolutions\n",
        "- Separateable convolutions\n",
        "- Average and max pooling\n",
        "- Global average and max pooling\n",
        "- LSTM, GRU (with built-in cuDNN acceleration)\n",
        "- BatchNormalization\n",
        "- Dropout\n",
        "- Attention\n",
        "- ConvLSTM2D\n",
        "- etc.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdrw7OppQ6At",
        "colab_type": "text"
      },
      "source": [
        "Keras는 디폴트로 좋은 설정값을 노출시키는 원칙을 따릅니다. 이렇게 해서, 필요한 인자값을 디폴트값으로 내버려두어도 대부분의 경우에서 잘 동작할 수 있게끔 해 줍니다. 예를 들어서, `LSTM` Layer는 디폴트로 직교 순환 행렬 초기화자(orthogonal recurrent matrix intializer)를 사용하고, 이는 forget 게이트의 편향값을 1로써 초기화 합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oq88tadFz8Z",
        "colab_type": "text"
      },
      "source": [
        "## `call` 메소드의 `training` 인자\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2NkTT0AQV8j",
        "colab_type": "text"
      },
      "source": [
        "몇 Layer, 특히 `BatchNormalization`과 `Dropout` Layer,는 학습과 추론단계에서 서로다른 동작방식을 가집니다. 이러한 종류의 Layer에 대해선, `call` 메소으의 (부울 형식인)`training` 인자를 노출시키는 것이 표준적인 관례입니다.\n",
        "\n",
        "`call` 메소드의 이 인자를 노출시킴으로써, 미리 제공되는 학습과 평가 반복문(예를 들어서 `fit` 메소드)이 해당 Layer를 학습과 추론에 대해서 옳바르게 사용할 수 있게 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ysXzHB5KJiLt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dropout(Layer):\n",
        "  \n",
        "  def __init__(self, rate):\n",
        "    super(Dropout, self).__init__()\n",
        "    self.rate = rate\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "    if training:\n",
        "      return tf.nn.dropout(inputs, rate=self.rate)\n",
        "    return inputs\n",
        "\n",
        "class MLPWithDropout(Layer):\n",
        "\n",
        "  def __init__(self):\n",
        "      super(MLPWithDropout, self).__init__()\n",
        "      self.linear_1 = Linear(32)\n",
        "      self.dropout = Dropout(0.5)\n",
        "      self.linear_3 = Linear(10)\n",
        "\n",
        "  def call(self, inputs, training=None):\n",
        "      x = self.linear_1(inputs)\n",
        "      x = tf.nn.relu(x)\n",
        "      x = self.dropout(x, training=training)\n",
        "      return self.linear_3(x)\n",
        "    \n",
        "mlp = MLPWithDropout()\n",
        "y_train = mlp(tf.ones((2, 2)), training=True)\n",
        "y_test = mlp(tf.ones((2, 2)), training=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyC7KfV-YcYS",
        "colab_type": "text"
      },
      "source": [
        "## 좀 더 함수형적으로 모델을 정의하기 위한 방법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxbEQANKQB6F",
        "colab_type": "text"
      },
      "source": [
        "딥 러닝 모델을 만들기 위해서, 항상 객체지향적 프로그래밍 방법을 사용할 필요는 없습니다. 아래의 예시처럼 Layer들은 함수형적으로도 조합이 가능합니다 (\"함수형 API\" 라고 부릅니다):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jiL-0N7sYc6X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# `Input` 객체를 사용해서, 입력의 shape(모양)과 dtype(데이터형)을 묘사합니다.\n",
        "# 딥러닝에서 이는 데이터형을 선언하는 방식입니다.\n",
        "# shape 인자는 샘플당 으로, 배치 크기를 포함하지 않습니다. \n",
        "# 함수형 API는 샘플당 변형을 정의하는데 집중합니다.\n",
        "# 생성하는 모델은 자동으로 샘플당 변형에 대한 배치를 고려합니다.\n",
        "# 따라서, 모델은 데이터의 배치마다 호출됩니다.\n",
        "inputs = tf.keras.Input(shape=(16,))\n",
        "\n",
        "# 이러한 \"데이터형\"의 객체에 대해서 Layer를 호출하고,\n",
        "# 호출 결과로 갱신된 (새로운 shape과 dtype을 가지는)\"데이터형\"을 반환합니다.\n",
        "x = Linear(32)(inputs) # 앞서 정의된 Linear Layer를 재사용 합니다.\n",
        "x = Dropout(0.5)(x)    # 앞서 정의된 Droptout Layer를 재사용 합니다.\n",
        "outputs = Linear(10)(x)\n",
        "\n",
        "# 함수형 `모델(Model)`은 입력과 출력을 명시하여 정의될 수 있습니다.\n",
        "# 모델은 다른것과 마찬가지로 스스로가 또 하나의 Layer가 됩니다.\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# 함수형 모델은 호출되기전, 이미 가중치를 가집니다.\n",
        "# 그 이유는 입력에 대한 shape을 `input`에서 사전에 정의했기 때문입니다.\n",
        "assert len(model.weights) == 4\n",
        "\n",
        "# 똑같은 데이터에 대해서, 모델을 다시 호출해 봅시다.\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vK5HqnT3Xgcz",
        "colab_type": "text"
      },
      "source": [
        "함수형 API는 하위 클래스를 만드는것 보다 더 간결하고, 여기엔 몇몇 부가적인 이점(일반적으로 함수형, 형 선언적 언어가 형 선언적이지 않은 객체지향 개발에 비해 가지는 이점과 동일)이 존재합니다. 하지만, 이는 Layer들의 DAGs를 정의하는데에만 사용될 수 있습니다. 재귀적인 네트워크는 `Layer`의 하위 클래스를 통해서 정의되어야 합니다.\n",
        "\n",
        "함수형 모델과 하위 클래스를 통해 정의된 모델의 주요 다른점은 [이곳](https://medium.com/tensorflow/what-are-symbolic-and-imperative-apis-in-tensorflow-2-0-dfccecb01021)에 설명되어 있습니다.\n",
        "\n",
        "[이곳](https://www.tensorflow.org/alpha/guide/keras/functional)을 방문해서, 함수형 API에 대해 좀 더 배워볼 수 있습니다.\n",
        "\n",
        "연구의 작업 흐름에서, 객체지향 모델과 함수형 모델을 섞어쓰는 자신을 종종 발견하게 될지도 모릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6p0KngmPTScu",
        "colab_type": "text"
      },
      "source": [
        "단일 입력과 출력을 가지는 Layer을 이용해서, 여러 층으로 구성된 모델에 대하여 `Sequential` 클래스를 사용할 수도 있습니다. 이 클래스는 Layer의 목록을 `Model`로 변환해 줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNhTY6frTaP2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Sequential\n",
        "\n",
        "model = Sequential([Linear(32), Dropout(0.5), Linear(10)])\n",
        "\n",
        "y = model(tf.ones((2, 16)))\n",
        "assert y.shape == (2, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cydf3i_FFXlh",
        "colab_type": "text"
      },
      "source": [
        "## Loss 클래스\n",
        "\n",
        "Keras는 넓은 범위의 미리 정의된 손실함수에 대한 Loss 클래스를 제공합니다. 이는 `BinaryCrossentropy`, `CategoricalCrossentropy`, `KLDivergence`등과 같은 것이 포함되며 다음과 같이 작동합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "019Nm1eWFaUO",
        "colab_type": "code",
        "outputId": "6eae11c7-ce23-4266-d13d-cc24a2790ee9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "bce = tf.keras.losses.BinaryCrossentropy()\n",
        "y_true = [0., 0., 1., 1.]  # 목표 (레이블)\n",
        "y_pred = [1., 1., 1., 0.]  # 예측 결과\n",
        "loss = bce(y_true, y_pred)\n",
        "print('손실:', loss.numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "손실: 11.522857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smbxFMGXY83U",
        "colab_type": "text"
      },
      "source": [
        "Loss 클래스는 상태를 가지지 않습니다. 즉, `__call__`의 출력은 입력에 대한 함수일 뿐입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RNLZsnswFbE_",
        "colab_type": "text"
      },
      "source": [
        "## Metric 클래스\n",
        "\n",
        "또한, Keras는 넓은 범위의 미리 정의된 평가지표 함수에 대한 Metric 클래스를 제공합니다. 이는 `BinaryAccuracy`, `AUC`, `FalsePositives`등과 같은것을 포함합니다.\n",
        "\n",
        "Loss와는 다르게, Metric은 상태를 가집니다. `update_state` 메소드를 사용해서 상태를 갱신하고, `result`를 사용해서 스칼라형태의 결과값을 요청할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4dUZkMWATKMC",
        "colab_type": "code",
        "outputId": "f2778aff-dd25-4322-d9f8-8242b5705160",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "m = tf.keras.metrics.AUC()\n",
        "m.update_state([0, 1, 1, 1], [0, 1, 0, 0])\n",
        "print('중간 결과: ', m.result().numpy())\n",
        "\n",
        "m.update_state([1, 1, 1, 1], [0, 1, 1, 0])\n",
        "print('최종 결과: ', m.result().numpy())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "중간 결과:  0.6666667\n",
            "최종 결과:  0.71428573\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "doUSrciie2Px",
        "colab_type": "text"
      },
      "source": [
        "내부 상태는 `metric.reset_states`에 의해 초기화될 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uwx7DjFBZ-C-",
        "colab_type": "text"
      },
      "source": [
        "`Metric` 클래스의 하위 클래스를 만들어서, 여러분만의 평가지표 함수를 손쉽게 만들수도 있습니다:\n",
        "\n",
        "- `__init__`내의 상태 변수를 생성합니다\n",
        "- `update_state`내에서 인자로써 주어진 `y_true`와 `y_pred`를 이용해서 변수를 갱신합니다\n",
        "- `result`내에서 평가지표의 결과를 반환합니다\n",
        "- `reset_states`내에서 상태를 초기화 합니다\n",
        "\n",
        "다음은 이 방법을 보여주기 위한 목적으로, `BinaryTruePositive` 평가지표에 대한 구현하고 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVByLrJyaBx_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BinaryTruePositives(tf.keras.metrics.Metric):\n",
        "\n",
        "  def __init__(self, name='binary_true_positives', **kwargs):\n",
        "    super(BinaryTruePositives, self).__init__(name=name, **kwargs)\n",
        "    self.true_positives = self.add_weight(name='tp', initializer='zeros')\n",
        "\n",
        "  def update_state(self, y_true, y_pred, sample_weight=None):\n",
        "    y_true = tf.cast(y_true, tf.bool)\n",
        "    y_pred = tf.cast(y_pred, tf.bool)\n",
        "\n",
        "    values = tf.logical_and(tf.equal(y_true, True), tf.equal(y_pred, True))\n",
        "    values = tf.cast(values, self.dtype)\n",
        "    if sample_weight is not None:\n",
        "      sample_weight = tf.cast(sample_weight, self.dtype)\n",
        "      sample_weight = tf.broadcast_weights(sample_weight, values)\n",
        "      values = tf.multiply(values, sample_weight)\n",
        "    self.true_positives.assign_add(tf.reduce_sum(values))\n",
        "\n",
        "  def result(self):\n",
        "    return self.true_positives\n",
        "\n",
        "  def reset_states(self):\n",
        "    self.true_positive.assign(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0PdvHdAdQl0",
        "colab_type": "text"
      },
      "source": [
        "## Optimizer 클래스 & 빠른 end-to-end 학습 반복문\n",
        "\n",
        "앞서 보여진 선형회귀 예제에서 작성한, 경사하강시 변수값을 직접 갱신하는 방법은 일반적으로 하지 않아도 됩니다. 보통은 `SGD`, `RMSprop`, 또는 `Adam`등과 같이 Keras에서 미리 제공되는 Optimizer 중 하나를 사용하면 됩니다.\n",
        "\n",
        "아래는 MNIST 데이터에 대해서, Loss, Metric 클래스와 Optimizer가 모두 함께 사용되는 예를 보여줍니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jNl1ykEdkj8",
        "colab_type": "code",
        "outputId": "f35594d0-3da4-44e6-d5ec-659d0ef86137",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        }
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "# 데이터셋를 준비합니다\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train[:].reshape(60000, 784).astype('float32') / 255\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# 간단한 분류를 위한 모델의 인스턴스를 만듭니다\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# 정수형 레이블을 인자로 받아들이는, 로지스틱 Loss의 인스턴스를 만듭니다\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 정확도에 대한 Metric의 인스턴스를 만듭니다\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Optimizer의 인스턴스를 만듭니다\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "# 데이터셋의 데이터 배치를 순회합니다\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  \n",
        "  # GradientTape 열어줍니다\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # 순방향 전파(forward)를 수행합니다\n",
        "    logits = model(x)\n",
        "\n",
        "    # 현재 배치에 대한 손실값을 측정합니다\n",
        "    loss_value = loss(y, logits)\n",
        "     \n",
        "  # 손실에 대한 가중치의 경사도를 계산합니다\n",
        "  gradients = tape.gradient(loss_value, model.trainable_weights)\n",
        "  \n",
        "  # 모델의 가중치를 갱신합니다\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_weights))\n",
        "\n",
        "  # 현재까지 수행된 전체에 대한 모델의 정확도를 갱신합니다\n",
        "  accuracy.update_state(y, logits)\n",
        "  \n",
        "  # 로그를 출력합니다\n",
        "  if step % 100 == 0:\n",
        "    print('단계(Step):', step)\n",
        "    print('마지막 단계(Step)의 손실:', float(loss_value))\n",
        "    print('지금까지 수행된 전체에 대한 정확도:', float(accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "단계(Step): 0\n",
            "마지막 단계(Step)의 손실: 2.3461451530456543\n",
            "지금까지 수행된 전체에 대한 정확도: 0.078125\n",
            "단계(Step): 100\n",
            "마지막 단계(Step)의 손실: 0.21598953008651733\n",
            "지금까지 수행된 전체에 대한 정확도: 0.833539605140686\n",
            "단계(Step): 200\n",
            "마지막 단계(Step)의 손실: 0.22803232073783875\n",
            "지금까지 수행된 전체에 대한 정확도: 0.8764770030975342\n",
            "단계(Step): 300\n",
            "마지막 단계(Step)의 손실: 0.11920223385095596\n",
            "지금까지 수행된 전체에 대한 정확도: 0.8954526782035828\n",
            "단계(Step): 400\n",
            "마지막 단계(Step)의 손실: 0.041876114904880524\n",
            "지금까지 수행된 전체에 대한 정확도: 0.9069903492927551\n",
            "단계(Step): 500\n",
            "마지막 단계(Step)의 손실: 0.07108411937952042\n",
            "지금까지 수행된 전체에 대한 정확도: 0.914857804775238\n",
            "단계(Step): 600\n",
            "마지막 단계(Step)의 손실: 0.05243276432156563\n",
            "지금까지 수행된 전체에 대한 정확도: 0.9213810563087463\n",
            "단계(Step): 700\n",
            "마지막 단계(Step)의 손실: 0.19157853722572327\n",
            "지금까지 수행된 전체에 대한 정확도: 0.9267118573188782\n",
            "단계(Step): 800\n",
            "마지막 단계(Step)의 손실: 0.2674962282180786\n",
            "지금까지 수행된 전체에 대한 정확도: 0.9306140542030334\n",
            "단계(Step): 900\n",
            "마지막 단계(Step)의 손실: 0.03551507741212845\n",
            "지금까지 수행된 전체에 대한 정확도: 0.9342397451400757\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIJYBrXoekXD",
        "colab_type": "text"
      },
      "source": [
        "`SparseCategoricalAccuracy` Metric 인스턴스를 재사용해서 테스트 반복문을 구현할 수 있습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sl6FKvqbeqX9",
        "colab_type": "code",
        "outputId": "f02f8dd6-3f1e-4d3d-a70a-b0befe329f0a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "x_test = x_test[:].reshape(10000, 784).astype('float32') / 255\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(128)\n",
        "\n",
        "accuracy.reset_states()  # 이 코드는 Metric의 내부 상태를 초기화 합니다\n",
        "\n",
        "for step, (x, y) in enumerate(test_dataset):\n",
        "  logits = model(x)\n",
        "  accuracy.update_state(y, logits)\n",
        "\n",
        "print('최종 테스트 정확도:', float(accuracy.result()))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "최종 테스트 정확도: 0.9557999968528748\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEP7jzC8YVWy",
        "colab_type": "text"
      },
      "source": [
        "## `add_loss` 메소드\n",
        "\n",
        "때로는 순방향 전파(forward) 수행 중 손실값을 계산해 보고 싶을 수 있습니다 (특히, 정규화(regularization) 손실에 대해서). Keras는 어느시점에서든지 손실값을 계산할 수 있게 해 주고, `add_loss` 메소드를 통해 이 손실값을 재귀적으로 계속 추적할 수 있게 해 줍니다.\n",
        "\n",
        "다음은 입력에 대한 L2 노름에 기반한 희소 정규화(regularization) 손실을 추가하는 Layer의 예를 보여줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbBVP--jYgHg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ActivityRegularization(Layer):\n",
        "  \"\"\"활성 희소 정규화 손실(activity sparsity regularization loss)을 생성하는 Layer 입니다\"\"\"\n",
        "  \n",
        "  def __init__(self, rate=1e-2):\n",
        "    super(ActivityRegularization, self).__init__()\n",
        "    self.rate = rate\n",
        "  \n",
        "  def call(self, inputs):\n",
        "    # 입력값에 기반하는\n",
        "    # `add_loss`를 사용해서 정규화 손실을 생성합니다\n",
        "    self.add_loss(self.rate * tf.reduce_sum(tf.square(inputs)))\n",
        "    return inputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4qoQk7abK5v",
        "colab_type": "text"
      },
      "source": [
        "`add_loss`를 이용해서 추가된 손실값은 `Layer` 또는 `Model`의 리스트형 속성인 `.losses`를 통해서 접근이 가능합니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlJc_4pbbQ2N",
        "colab_type": "code",
        "outputId": "a9501d61-49c2-4069-f0ac-43951c3b6fe7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "class SparseMLP(Layer):\n",
        "  \"\"\"희소 정규화 손실을 가지는 선형 계층을 쌓아올린 Layer 입니다\"\"\"\n",
        "\n",
        "  def __init__(self, output_dim):\n",
        "      super(SparseMLP, self).__init__()\n",
        "      self.dense_1 = layers.Dense(32, activation=tf.nn.relu)\n",
        "      self.regularization = ActivityRegularization(1e-2)\n",
        "      self.dense_2 = layers.Dense(output_dim)\n",
        "\n",
        "  def call(self, inputs):\n",
        "      x = self.dense_1(inputs)\n",
        "      x = self.regularization(x)\n",
        "      return self.dense_2(x)\n",
        "    \n",
        "\n",
        "mlp = SparseMLP(1)\n",
        "y = mlp(tf.ones((10, 10)))\n",
        "\n",
        "print(mlp.losses)  # float32 자료형의 단일 스칼라값을 가지는 리스트 입니다"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: id=186023, shape=(), dtype=float32, numpy=0.3541786>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkI3GA2TbWvY",
        "colab_type": "text"
      },
      "source": [
        "이 손실값들은 순방향 전파(forward)의 시작점에 있는 최상위 Layer로부터 초기화되며 축적되지 않습니다. 따라서 `layer.losses`는 항상 마지막 순방향 전파동안 생성된 손실값만을 가지게 됩니다. 학습 반복문을 작성할 때, 일반적으로 경사도 계산 이전에 이 손실값들에 대한 합산을 수행합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2m0xNYGEbZe2",
        "colab_type": "code",
        "outputId": "9a1c60f7-8f79-4d27-8cb2-1a592736917f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# *마지막* 순방향 전파에 해당하는 손실값들 입니다\n",
        "mlp = SparseMLP(1)\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1\n",
        "mlp(tf.ones((10, 10)))\n",
        "assert len(mlp.losses) == 1  # 축적되지 않습니다\n",
        "\n",
        "# 이 손실값들을 학습 반복문에서 사용하는법을 보여줍니다\n",
        "\n",
        "# 데이터셋을 준비합니다\n",
        "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (x_train.reshape(60000, 784).astype('float32') / 255, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# 새로운 MLP를 만듭니다\n",
        "mlp = SparseMLP(10)\n",
        "\n",
        "# Loss와 Optimizer를 만듭니다\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "for step, (x, y) in enumerate(dataset):\n",
        "  with tf.GradientTape() as tape:\n",
        "\n",
        "    # 순방향 전파를 수행합니다\n",
        "    logits = mlp(x)\n",
        "\n",
        "    # 현재 배치에 대한 외부의 손실값을 계산합니다\n",
        "    loss = loss_fn(y, logits)\n",
        "    \n",
        "    # 순방향 전파시 생성된 손실값을 더해줍니다 \n",
        "    loss += sum(mlp.losses)\n",
        "     \n",
        "    # 해당 손실에 대한 가중치의 경사도를 계산합니다\n",
        "    gradients = tape.gradient(loss, mlp.trainable_weights)\n",
        "  \n",
        "  # 모델의 가중치를 갱신합니다\n",
        "  optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
        "  \n",
        "  # 로그를 출력합니다\n",
        "  if step % 100 == 0:\n",
        "    print(step, float(loss))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 3.8689327239990234\n",
            "100 2.310004234313965\n",
            "200 2.2619950771331787\n",
            "300 2.1776585578918457\n",
            "400 2.0352156162261963\n",
            "500 1.9956769943237305\n",
            "600 2.027062177658081\n",
            "700 1.9948246479034424\n",
            "800 2.0250766277313232\n",
            "900 1.7551041841506958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zecO65f2Ph7O",
        "colab_type": "text"
      },
      "source": [
        "## 자세한 end-to-end 예제: Variational AutoEncoder (VAE)\n",
        "\n",
        "기초적인 내용의 공부를 잠시 미뤄두고, 약간 더 어려운 예제를 살펴보고 싶다면, [여기에 소개된 VAE](https://www.tensorflow.org/guide/keras/custom_layers_and_models#putting_it_all_together_an_end-to-end_example)에 대한 구현의 예제를 확인해 보시기 바랍니다. 이는 여러분이 지금까지 배워왔던 모든것의 내용을 담고 있습니다:\n",
        "\n",
        "- `Layer`의 하위 클래스를 만드는것\n",
        "- 재귀적으로 Layer를 구성하는것\n",
        "- Loss 및 Metric 클래스에 대한것\n",
        "- `add_loss`\n",
        "- `GradientTape`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7V0yRrfYFuVT",
        "colab_type": "text"
      },
      "source": [
        "## 미리 정의된 학습 반복문을 사용하는것"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNwAjgEXPpnP",
        "colab_type": "text"
      },
      "source": [
        "간단한 케이스에 대해서 조차 여러분이 스스로 저수준의 학습 반복문을 매번 작성해야 한다면, 이는 어리석은 일일지도 모릅니다. Keras는 미리 정의된 학습 반복문을 `Model` 클래스에서 제공합니다. 사용하고자 한다면, `Model`의 하위 클래스를 만들거나 `Functional(함수형)` 또는 `Sequential(순차형)` 모델을 생성하면 됩니다.\n",
        "\n",
        "이를 보여주기 위해서, 앞서 만들어둔 MNIST의 예를 재사용 해 보겠습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omdNf2x4jovv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 데이터셋을 준비합니다\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
        "\n",
        "# 간단한 분류모델의 인스턴스를 만듭니다\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# 정수형 레이블을 인자로 받아들이는, 로지스틱 Loss의 인스턴스를 만듭니다\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 정확도에 대한 Metric의 인스턴스를 만듭니다\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Optimizer의 인스턴스를 만듭니다\n",
        "optimizer = tf.keras.optimizers.Adam()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYU5PkaLiriO",
        "colab_type": "text"
      },
      "source": [
        "가장 첫 번째로, `compile` 메소드를 호출하여 Optimizer, Loss, 모니터링하기 위한 Metric을 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eo6GhvzjJjdb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=optimizer, loss=loss, metrics=[accuracy])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98zDjMPej06U",
        "colab_type": "text"
      },
      "source": [
        "그리고 나선 `fit` 메소드를 호출하고, 이 때 데이터를 전달해 줍니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-OdztL4nj4ed",
        "colab_type": "code",
        "outputId": "6fc4eb46-6dd9-475c-ef60-343f1e6c5937",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "model.fit(dataset, epochs=3)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "938/938 [==============================] - 9s 10ms/step - loss: 0.2215 - sparse_categorical_accuracy: 0.9346\n",
            "Epoch 2/3\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.0863 - sparse_categorical_accuracy: 0.9730\n",
            "Epoch 3/3\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.0542 - sparse_categorical_accuracy: 0.9830\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0e203d7e10>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZc2ss8qln1p",
        "colab_type": "text"
      },
      "source": [
        "이게 끝입니다! 이제는 테스트를 해 봅시다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0-XQbxOlqB3",
        "colab_type": "code",
        "outputId": "c0ec4ea4-74bc-454c-ad2f-a0fc1f3394c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "x_test = x_test[:].reshape(10000, 784).astype('float32') / 255\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_dataset = test_dataset.batch(128)\n",
        "\n",
        "loss, acc = model.evaluate(test_dataset)\n",
        "print('손실:', loss, '정확도:', acc)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "79/79 [==============================] - 0s 4ms/step - loss: 0.0951 - sparse_categorical_accuracy: 0.9720\n",
            "손실: 0.09513797635828511 정확도: 0.972\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkmbEn55nV7y",
        "colab_type": "text"
      },
      "source": [
        "`fit`이 수행되는 동안 검증용 데이터셋에 대한 Loss와 Metric을 모니터링 하는것 또한 가능합니다.\n",
        "\n",
        "또한, Numpy형의 배열에 대해서도 직접적으로 `fit`을 호출할 수 있습니다. 따라서 데이터셋에 대한 변환이 필요 없습니다:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q8UuNivngVe",
        "colab_type": "code",
        "outputId": "83f2ff29-cd0d-4218-e4d2-30351b792c9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
        "\n",
        "num_val_samples = 10000\n",
        "x_val = x_train[-num_val_samples:]\n",
        "y_val = y_train[-num_val_samples:]\n",
        "x_train = x_train[:-num_val_samples]\n",
        "y_train = y_train[:-num_val_samples]\n",
        "\n",
        "# 간단한 분류모델의 인스턴스를 만듭니다\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# 정수형 레이블을 인자로 받아들이는, 로지스틱 Loss의 인스턴스를 만듭니다\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 정확도에 대한 Metric의 인스턴스를 만듭니다\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Optimizer의 인스턴스를 만듭니다\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])\n",
        "model.fit(x_train, y_train,\n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=3,\n",
        "          batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "50000/50000 [==============================] - 5s 102us/sample - loss: 0.2427 - sparse_categorical_accuracy: 0.9282 - val_loss: 0.1173 - val_sparse_categorical_accuracy: 0.9649\n",
            "Epoch 2/3\n",
            "50000/50000 [==============================] - 5s 92us/sample - loss: 0.0931 - sparse_categorical_accuracy: 0.9717 - val_loss: 0.1063 - val_sparse_categorical_accuracy: 0.9682\n",
            "Epoch 3/3\n",
            "50000/50000 [==============================] - 5s 92us/sample - loss: 0.0604 - sparse_categorical_accuracy: 0.9816 - val_loss: 0.0820 - val_sparse_categorical_accuracy: 0.9767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0ddc191208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88ExjKfCo7aP",
        "colab_type": "text"
      },
      "source": [
        "## Callbacks\n",
        "\n",
        "`fit`이 가지는 간단하지만 훌륭한 기능 중 하나로, [callbacks](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/)을 사용해서 학습과 평가 도중 일어나는 일에 대한 사용자 정의화가 가능합니다.\n",
        "\n",
        "Callback은 객체의 한 종류로, 학습 중간 중간에 호출(예를들어, 매 배치마다 또는 매 epoch마다) 되며 어떤 작업을 수행합니다.\n",
        "\n",
        "미리 정의된 여러가지 Callback이 존재합니다. `ModelCheckpoint`는 학습도중 매 epoch마다 모델을 저장하고, `EarlyStopping`은 검증용 평가지표(metrics)가 향상되지 않을 때 학습을 중단시킵니다.\n",
        "\n",
        "물론, 손쉽게 [여러분만의 callback을 작성할 수도 있습니다](https://www.tensorflow.org/guide/keras/custom_callback).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAylVdYJqcZ3",
        "colab_type": "code",
        "outputId": "e33395c0-832c-4f39-9d05-c3c74f8f3dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "# 간단한 분류모델의 인스턴스를 만듭니다\n",
        "model = tf.keras.Sequential([\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(256, activation=tf.nn.relu),\n",
        "  layers.Dense(10)\n",
        "])\n",
        "\n",
        "# 정수형 레이블을 인자로 받아들이는, 로지스틱 Loss의 인스턴스를 만듭니다\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# 정확도에 대한 Metric의 인스턴스를 만듭니다\n",
        "accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
        "\n",
        "# Optimizer의 인스턴스를 만듭니다\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "model.compile(optimizer=optimizer,\n",
        "              loss=loss,\n",
        "              metrics=[accuracy])\n",
        "\n",
        "# 몇가지 Callback의 인스턴스를 만듭니다\n",
        "callbacks = [tf.keras.callbacks.EarlyStopping(),\n",
        "             tf.keras.callbacks.ModelCheckpoint(filepath='my_model.keras',\n",
        "                                                save_best_only=True)]\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          validation_data=(x_val, y_val),\n",
        "          epochs=30,\n",
        "          batch_size=64,\n",
        "          callbacks=callbacks)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "50000/50000 [==============================] - 5s 100us/sample - loss: 0.2408 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.1155 - val_sparse_categorical_accuracy: 0.9666\n",
            "Epoch 2/30\n",
            "50000/50000 [==============================] - 5s 91us/sample - loss: 0.0928 - sparse_categorical_accuracy: 0.9710 - val_loss: 0.1049 - val_sparse_categorical_accuracy: 0.9682\n",
            "Epoch 3/30\n",
            "50000/50000 [==============================] - 5s 90us/sample - loss: 0.0614 - sparse_categorical_accuracy: 0.9807 - val_loss: 0.0958 - val_sparse_categorical_accuracy: 0.9725\n",
            "Epoch 4/30\n",
            "50000/50000 [==============================] - 4s 89us/sample - loss: 0.0450 - sparse_categorical_accuracy: 0.9857 - val_loss: 0.0838 - val_sparse_categorical_accuracy: 0.9753\n",
            "Epoch 5/30\n",
            "50000/50000 [==============================] - 5s 92us/sample - loss: 0.0364 - sparse_categorical_accuracy: 0.9883 - val_loss: 0.0946 - val_sparse_categorical_accuracy: 0.9734\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f0ddbcaccf8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxINLLGitX_n",
        "colab_type": "text"
      },
      "source": [
        "# 작별 인사\n",
        "\n",
        "저는 이 가이드가 여러분에게 TensorFlow2.0과 Keras로 무엇을 할 수 있는지 알려주는 좋은 오버뷰가 되길 희망합니다!\n",
        "\n",
        "TensorFlow와 Keras는 단일 작업 흐름만을 대변하는게 아니라는 것을 기억하세요. 사용성과 유연성이라는 트레이드오프를 가지는 여러 범위의 작업흐름을 지원합니다. 예를 들어서, `fit` 메소드를 사용하는것이 사용자정의 학습 반복문을 작성하는것보다 훨씬 쉽지만, `fit`은 연구에서 필요한 미세한 조절이 가능한 수준까지를 제공하진 못합니다.\n",
        "\n",
        "따라서, 여러분의 일에 맞는 알맞은 툴을 사용하세요!\n",
        "\n",
        "Keras의 중심이 되는 원칙은 \"복잡도의 점진적인 공개\" 입니다. 매우 쉽게 시작할 수 있고, 점점 더 많은 부분을 밑바닥에서 부터 구현해야 하는 작업흐름에 대해서 점진적으로 좀 더 깊이 들여다보고, 그렇게함으로써 완전한 제어를 할 수 있게 됩니다.\n",
        "\n",
        "\n",
        "이 사실은 모델의 정의와 모델의 학습 모두에 적용되는 것입니다.\n",
        "\n",
        "![모델의 정의: 작업 흐름의 범위](https://drive.google.com/uc?export=view&id=1bTHrw-OXaKqnJI-DVGG04suWHVOl8UVj)\n",
        "\n",
        "![모델의 학습: 작업 흐름의 범위](https://drive.google.com/uc?export=view&id=1a6oMJ9IKyMg19JX7NkihyOfzdiJo5Mpq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfO_uy61upRm",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## 이 다음으로 보면 좋을만한 것들\n",
        "\n",
        "이 가이드 다음으로, 여러분이 관심을 가질만한 주제가 더 있습니다:\n",
        "\n",
        "- [저장과 직렬화](https://www.tensorflow.org/guide/keras/save_and_serialize)\n",
        "- [다중 GPUs에서의 분산 학습](https://www.tensorflow.org/guide/distributed_training)\n",
        "- [임베디드 시스템이나 안드로이드 개발에 활용하기 위해 모델을 TFLite로 내보내기](https://www.tensorflow.org/lite/convert/python_api#converting_a_keras_model_)\n",
        "- [브라우져에서의 개발에 활용하기 위해 모델을 TensorFlow.js로 내보내기](https://www.tensorflow.org/js/tutorials/conversion/import_keras)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FelcNL4gO4mP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}